{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0577d39b",
   "metadata": {},
   "source": [
    "## Reverse dictionary in Spanish: Part 1\n",
    "\n",
    "Every once in a while I'm trying to remember a word and I have a vague memory of its definition. Googling that definition  to try to find the word  is most of the time not very useful. I thought a reverse dictionary, where you search a definition and get word that matches it,  would be useful for this case. Turns out is a common term to refer to this kind of task. With the rise of LLM and general NLP advances, seems like something like this should be easy nowadays, but as it tends to happen doing so in spanish is not as easy. In this first part, I will build the most basic reverse dictionary using available word embbedings. In the next parts will venture into fine-tuning a new neural net, or straight up using something like the newly fine-tuned llama2 in spanish to try make it work for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef793b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import fastbook\n",
    "import fastai\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4c531a",
   "metadata": {},
   "source": [
    "Exactly what embbedings to use I wasn't sure, so I picked the vector formatted version of Word2Vec embeddings from SBWC from [here](https://github.com/dccuchile/spanish-word-embeddings#word2vec-embeddings-from-sbwc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ded5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip it. This works with WSL/linux etc. Won't work (I think) if you're just running jupyter from windows cmd\n",
    "# this will also delete the bz2. If you want to keep it use -dk instead of -d\n",
    "# !bzip2 -d data/SBW-vectors-300-min5.txt.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252cc325",
   "metadata": {},
   "source": [
    "The unzipped file is almost 3gb, so we import a few examples to check it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a25f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gensim Keyvectors to read the embbedings \n",
    "wordvectors_file_vec = 'data/SBW-vectors-300-min5.txt'\n",
    "cantidad = 10000\n",
    "wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)\n",
    "# wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f00a2",
   "metadata": {},
   "source": [
    "Embeddings like this really aren't anything more than a mapping between keys and vectors. Basically each word has a bunch of numbers that correspond to it, and that we can interpret as a sort of meaning: semantically similar words should have similar numbers, very different words should have very different numbers.\n",
    "\n",
    "Most of the time can be a bit confusing how to index into this object. What words are in this sample? How do the numbers look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "268f01e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[-0.029648  0.011336  0.019949 -0.088832 -0.025225  0.056844  0.025473  0.014068  0.163694 -0.067154]\n",
      "de\n",
      "3278\n"
     ]
    }
   ],
   "source": [
    "print(wordvectors[0].shape) # So, vector of 300 for each word\n",
    "print(wordvectors[0][:10]) # A few examples of whatever the word 0 is\n",
    "\n",
    "# \n",
    "print(wordvectors.index_to_key[0]) # You can find what word is the word 0 with this method\n",
    "print(wordvectors.key_to_index['reina']) # Or search for the index of a particular word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de66d963",
   "metadata": {},
   "source": [
    "Since I wanted to eventually turn this into a small gradio app to share, I figured using the whole 3 gb of embbedings is overkill and also, surely there's a lot of words that I don't really need there (like places, names, etc.). Ideally I would have a database with words useful for spellcheking in spanish so I could filter it, but I couldn't find one. I suppose I could straight up scrape RAE or wordreference but I don't think they allow and it might take quite a while to get that going.\n",
    "\n",
    "I ended up deciding to use [Wikcionario](https://es.wiktionary.org/wiki/Wikcionario:Portada) the only really open dictionary in \n",
    "spanish I could find. Kind of a problem because they don't just provide a list of words ready to use, so I had to go through the eswiki dump and see what files I could use. I went for eswiki-20230820-pages-articles.xml.bz2 that you can find [here](https://dumps.wikimedia.org/eswiki/20230820/).  Other options might be :\n",
    "\n",
    "* [Spanish Wordnet](http://grial.edu.es/web/es/descargas/): Might be good too, and honestly didn't find it when I was first doing this sooooo I'll check it out later. Not sure of their terms, and also seems like its just a translation of wordnet.\n",
    "* [Freeling](https://nlp.lsi.upc.edu/freeling/index.php/node/10) Could not find where to get the dictionary, which has 500k+ words. I think its just [here](https://github.com/TALP-UPC/FreeLing/tree/master/data/es/dictionary/entries) but seemed like it was going to take a while to figure it out.\n",
    "* [Apertium](https://github.com/apertium) This might be the best alternative really. File is [here](https://repositori.upf.edu/handle/10230/17123) though unzipping and figuring where exactly is what I want is also an issue.\n",
    "\n",
    "Anyways I went ahead and downloaded the full xml, hoping to parse and extract the titles of each article to use for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe067a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bzip2 -d data/eswiktionary-20230820-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da02969",
   "metadata": {},
   "source": [
    "Problem again is, I don't really need the whole site's xml data, just the words of the articles in there. So load the XML and filter for the appropiate tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecaa87b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'http://www.mediawiki.org/xml/export-0.10/'}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse('data/eswiktionary-20230820-pages-articles.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Had to this because I wasn't getting any of the titles, turns out I needed the namespace for it\n",
    "namespaces = {elem.tag.split('}')[0].strip('{') for elem in root.iter() if '}' in elem.tag}\n",
    "print(namespaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ffe2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespaces = {'ns': 'http://www.mediawiki.org/xml/export-0.10/'}  # the namespace mapping\n",
    "titles = [elem.text for elem in root.findall('.//ns:title', namespaces)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d190c9d",
   "metadata": {},
   "source": [
    "It works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6e00d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MediaWiki:Category',\n",
       " 'MediaWiki:Helppage',\n",
       " 'MediaWiki:Wikititlesuffix',\n",
       " 'MediaWiki:Bugreportspage',\n",
       " 'Plantilla:Sitesupportpage',\n",
       " 'MediaWiki:Qbspecialpages',\n",
       " 'Plantilla:Fromwikipedia',\n",
       " 'MediaWiki:Postcomment',\n",
       " 'Plantilla:Gnunote',\n",
       " 'MediaWiki:Developertitle',\n",
       " 'MediaWiki:Developertext',\n",
       " 'MediaWiki:Sitesubtitle',\n",
       " 'MediaWiki:Noconnect',\n",
       " 'MediaWiki:Missingarticle',\n",
       " 'MediaWiki:Perfdisabled',\n",
       " 'MediaWiki:Perfdisabledsub',\n",
       " 'MediaWiki:Whitelistedittitle',\n",
       " 'MediaWiki:Whitelistreadtitle',\n",
       " 'MediaWiki:Whitelistreadtext',\n",
       " 'MediaWiki:Whitelistacctitle',\n",
       " 'MediaWiki:Whitelistacctext',\n",
       " 'MediaWiki:Newarticletext',\n",
       " 'MediaWiki:Anontalkpagetext',\n",
       " 'MediaWiki:Noarticletext',\n",
       " 'Plantilla:Sectionedit',\n",
       " 'Plantilla:Commentedit',\n",
       " 'MediaWiki:Protectedpagewarning',\n",
       " 'MediaWiki:Revhistory',\n",
       " 'MediaWiki:Loadhist',\n",
       " 'Plantilla:Searchhelppage']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5afadfd",
   "metadata": {},
   "source": [
    "It sucks. Obviously a lot of titles are stuff that I don't need. Filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23713496",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_titles = [title for title in titles if ':' not in title] # this to eliminate wiktionary specific titles\n",
    "filtered_titles = [title for title in filtered_titles if title.islower()] # upper case means mostly places, names and such\n",
    "filtered_titles = [title for title in filtered_titles if len(title.split()) == 1] # only care for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dba0477d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['japonés',\n",
       " 'hiragana',\n",
       " 'katakana',\n",
       " 'alemán',\n",
       " 'catalán',\n",
       " 'mayo',\n",
       " 'domingo',\n",
       " 'hoy',\n",
       " 'gurú',\n",
       " 'francés']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_titles[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955be87d",
   "metadata": {},
   "source": [
    "Much better. I think. Next issue is filtering the words from the wordvector to words that exist in the filtered titles from wikcionario. So, read the whole embbeding file and then filter. This is real slow on my local pc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90be2c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec)\n",
    "words = wordvectors.index_to_key[0:len(wordvectors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "849e294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aer = [word for word in words if word in filtered_titles] <- this takes forever, don't try\n",
    "filtered_titles_set = set(filtered_titles)\n",
    "aer = [word for word in words if word in filtered_titles_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "291af2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000653\n",
      "856904\n",
      "173274\n"
     ]
    }
   ],
   "source": [
    "print(len(words)) # number of words in the embbeding \n",
    "print(len(filtered_titles_set)) # number of words in the final filter of the wiktionary\n",
    "print(len(aer)) # final number of words the smaller embedding matrix i'll keep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25626e8e",
   "metadata": {},
   "source": [
    "Not sure if it makes sense that the difference is such, but for now I don't care. Filter and save the smaller model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89ed3140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just trying to slice the wordvector\n",
    "words_to_keep = aer\n",
    "smaller_model = wordvectors.vectors_for_all(words_to_keep)\n",
    "smaller_model.save_word2vec_format(\"smaller_model_spa.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f534f1",
   "metadata": {},
   "source": [
    "### Examples!\n",
    "\n",
    "So now we can use some of the methods and try this first basic version of a spanish reverse dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80083000",
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_model.most_similar_cosmul(positive=['angustia','esperar'])\n",
    "\n",
    "#A type of vehicle that goes under the water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "674f1410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reconcome', 0.2180447280406952),\n",
       " ('apachurra', 0.2174457609653473),\n",
       " ('empezás', 0.21651658415794373),\n",
       " ('causaste', 0.2164955586194992),\n",
       " ('exploté', 0.2155492752790451),\n",
       " ('comentabas', 0.2149752825498581),\n",
       " ('desconfié', 0.21489164233207703),\n",
       " ('agüitado', 0.21468758583068848),\n",
       " ('recordá', 0.213822603225708),\n",
       " ('aborrecerse', 0.21345187723636627)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1: distress because you forgot something\n",
    "smaller_model.most_similar_cosmul(positive=['angustia', 'porque', 'se', 'te', 'olvidó', 'algo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6af7c3",
   "metadata": {},
   "source": [
    "Didn't know the word \"reconcome\", but turns out rae says [reconcomer](https://dle.rae.es/reconcomer) means \"Dicho de un problema, una preocupación\" (about a problem, a worry). It actually works pretty well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40bf9b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cuarteando', 0.3146556615829468),\n",
       " ('cachamos', 0.31217625737190247),\n",
       " ('basculen', 0.3110094964504242),\n",
       " ('erran', 0.3109343945980072),\n",
       " ('que', 0.3109077513217926),\n",
       " ('avecinarían', 0.3102854788303375),\n",
       " ('enfermarían', 0.3097950518131256),\n",
       " ('acongojantes', 0.30800893902778625),\n",
       " ('destronadas', 0.30754488706588745),\n",
       " ('baqueteadas', 0.3073866665363312)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2: fear of heights\n",
    "smaller_model.most_similar_cosmul(positive=['miedo', 'a', 'las', 'alturas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281a4b0",
   "metadata": {},
   "source": [
    "Not even close, but without stopwords acrofobia actually appears in the list, which means fear of heights. Didn't know the word either and I usually thought of \"vértigo\" when thinking of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58e5e434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('basculen', 0.5672526359558105),\n",
       " ('enervadas', 0.5599164962768555),\n",
       " ('encrespe', 0.5534656047821045),\n",
       " ('ensuciarán', 0.5530100464820862),\n",
       " ('apeteciendo', 0.5520198345184326),\n",
       " ('topábamos', 0.5518502593040466),\n",
       " ('acrofobia', 0.5518409609794617),\n",
       " ('importunase', 0.5515409708023071),\n",
       " ('sobresaltando', 0.5505372881889343),\n",
       " ('solazo', 0.549772322177887)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 3\n",
    "smaller_model.most_similar_cosmul(positive=['miedo', 'alturas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4786a",
   "metadata": {},
   "source": [
    "Makes sense really. An embedding like this means each words really only has one meaning, when we know a lot of them can have multiple meanings. So adding connectors, stopwords and such will probably make the search worse. Something to think about when wrapping it all in the gradio app\n",
    "\n",
    "The next example is kind of funny. Was hoping it would find submarine (\"vehicle that goes under water\") but found something else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d054d922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('desbarrancada', 0.16990652680397034),\n",
       " ('enfilen', 0.1675097644329071),\n",
       " ('gritadera', 0.1669069081544876),\n",
       " ('cremaron', 0.16604605317115784),\n",
       " ('encostalado', 0.1657562106847763),\n",
       " ('servicialmente', 0.16530342400074005),\n",
       " ('arroyar', 0.1648721992969513),\n",
       " ('desenmascaro', 0.16470250487327576),\n",
       " ('escaldó', 0.1644110232591629),\n",
       " ('citroneta', 0.16413670778274536)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 4\n",
    "smaller_model.most_similar_cosmul(positive=['vehiculo', 'que', 'anda', 'bajo', 'el', 'agua'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4b5ee7",
   "metadata": {},
   "source": [
    "Desbarrancada means fall, as in, from a cliff. So that vehicle is under water but for sure not functioning. Finally, an example that really didn't work (\"group of wolves that roam together\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5b8e2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('albertosaurios', 0.17380031943321228),\n",
       " ('camionetita', 0.17278659343719482),\n",
       " ('destazando', 0.1716463416814804),\n",
       " ('acicalaban', 0.17020738124847412),\n",
       " ('desenmascaro', 0.16754880547523499),\n",
       " ('rechistara', 0.16714678704738617),\n",
       " ('atónicos', 0.16713115572929382),\n",
       " ('toreamos', 0.16500285267829895),\n",
       " ('convalecían', 0.16470967233181),\n",
       " ('flipados', 0.16468177735805511)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 4\n",
    "smaller_model.most_similar_cosmul(positive=['grupo', 'de', 'lobos', 'que', 'andan', 'juntos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1fdee9",
   "metadata": {},
   "source": [
    "Is albertosaurio an old dude named alberto or an actual dinosaur? Don't know but for sure has nothing to do with a pack of wolves. No combination of those words actually makes to have \"manada\" in the list. Though it does turn out that if you use \"rebaño\" as a positive word, it does find \"manada\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd804da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('montescos', 0.5416086912155151),\n",
       " ('lobo', 0.5249671936035156),\n",
       " ('andariegos', 0.5227840542793274),\n",
       " ('albertosaurios', 0.5224950909614563),\n",
       " ('cuellilargos', 0.5204229950904846),\n",
       " ('azotadores', 0.5189481973648071),\n",
       " ('destazando', 0.5181227326393127),\n",
       " ('boleando', 0.5153393745422363),\n",
       " ('arponeando', 0.5151293277740479),\n",
       " ('arponeados', 0.5130795836448669)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smaller_model.most_similar_cosmul(positive=['grupo', 'lobos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d78876eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('manada', 0.650048553943634),\n",
       " ('rebaños', 0.643172025680542),\n",
       " ('cabras', 0.6415314674377441),\n",
       " ('ovejas', 0.6396838426589966),\n",
       " ('lobo', 0.6271942853927612),\n",
       " ('lobeznos', 0.6143544912338257),\n",
       " ('manadas', 0.6129717826843262),\n",
       " ('corderillos', 0.6069217324256897),\n",
       " ('salvajes', 0.6065280437469482),\n",
       " ('zorros', 0.6042017936706543)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smaller_model.most_similar_cosmul(positive=['lobos', 'rebaño'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c6c837",
   "metadata": {},
   "source": [
    "### Closing thoughts\n",
    "\n",
    "So this works pretty ok out of the box. If you wanted to just run it locally you could use the whole embedding matrix and do nothing more than call the methods for it. The more annoying part was finding open resources for spanish really. Still, a lot of improvements could be made for later parts:\n",
    "\n",
    "* Using actual definitions: As of now, we used the meaning of words estimated for a different task like next word prediction, but we really aren't using the fact that the mapping we really want to take advantage of is definition -> word. So using the embeddings to train for a different task seems like the next obvious step\n",
    "* Multiple word meanings: The fact that each word is mapped to one vector means that multiple meanings get lost, which I believe is part of the reasons why removing some words helps the search in this example. Context gets lost\n",
    "\n",
    "So next up should be, figuring out how to use contex (multisense embedding maybe?) and then some architecture to train the task we actually care about. For now, the most immediate thing to do is wrap the method used in a function so as to strip the string from mostly useless words to put up the gradio app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c7cda7",
   "metadata": {},
   "source": [
    "### Removing stopwords and filtering for gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efcef09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/imauriacaf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/imauriacaf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0256dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = nltk.data.load('nltk:reverse_dictionary/tokenizers/punkt/spanish.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed87e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sent = \"grupo de lobos que andan juntos\"\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b727ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('stop_words.pkl', 'wb') as f:\n",
    "    pickle.dump(stop_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd768eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(x):\n",
    "    word_tokens = word_tokenize(x)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return filtered_sentence\n",
    "\n",
    "def reverse_dictionary(definition):\n",
    "    words = filter_words(definition)\n",
    "    list_similar = smaller_model.most_similar_cosmul(positive= words)\n",
    "    return list_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2968aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors_file_vec = 'data/smaller_model_spa.txt'\n",
    "smaller_model = KeyedVectors.load_word2vec_format(wordvectors_file_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43af3460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arroyar', 0.41619226336479187),\n",
       " ('despapaye', 0.41503772139549255),\n",
       " ('pepenando', 0.4141307771205902),\n",
       " ('parapete', 0.41403526067733765),\n",
       " ('mareta', 0.4119090437889099),\n",
       " ('rentábamos', 0.41015368700027466),\n",
       " ('servicialmente', 0.40943971276283264),\n",
       " ('empoza', 0.4043053090572357),\n",
       " ('fregó', 0.4042477607727051),\n",
       " ('parquea', 0.4041784405708313)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary(\"vehiculo que anda en el agua\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c956501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imauriacaf/mambaforge/envs/fastai/lib/python3.11/site-packages/gradio/inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/home/imauriacaf/mambaforge/envs/fastai/lib/python3.11/site-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/home/imauriacaf/mambaforge/envs/fastai/lib/python3.11/site-packages/gradio/deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://924c3b97bcee8ca03b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://924c3b97bcee8ca03b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "# labels = learn.dls.vocab\n",
    "gr.Interface(fn = reverse_dictionary, \n",
    "             inputs = gr.inputs.Textbox(lines=5, placeholder=\"Enter your text here...\"), \n",
    "             outputs= \"text\" ).launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc664b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
